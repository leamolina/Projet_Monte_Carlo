# ğŸ“ Projet MCTS & RL - Recherche Monte Carlo et Jeux

## ğŸ¯ Objectif gÃ©nÃ©ral

Comparer diffÃ©rentes approches de dÃ©cision (MCTS et Q-learning) sur plusieurs environnements Gymnasium. Proposer une amÃ©lioration issue dâ€™un article de recherche externe dans une phase finale.

---

## ğŸ§© Environnements utilisÃ©s

| Jeu                 | Type                     | Rendu "human" | IntÃ©rÃªt pÃ©dagogique                            |
|---------------------|--------------------------|---------------|-------------------------------------------------|
| **FrozenLake-v1**   | Discret, stochastique    | âœ…             | Environnement simple, idÃ©al pour debug + RL     |
| **CartPole-v1**     | Discret, dynamique       | âœ…             | Apprentissage dynamique, scores continus        |
| **LunarLander-v2**  | Discret, plus complexe   | âœ…             | DÃ©fis stratÃ©giques et contrÃ´le spatial          |

---

## ğŸ”¶ Phase 1 â€” Flat Monte Carlo Tree Search

### âœ… Objectif
Utiliser **Flat MCTS**, câ€™est-Ã -dire une recherche par playouts depuis chaque action de la racine.

### ğŸ“Œ TÃ¢ches
- ImplÃ©mentation du Flat MCTS
- Ã‰valuation de chaque action via plusieurs playouts alÃ©atoires
- ExÃ©cution sur 3 jeux Gymnasium

### ğŸ”¬ MÃ©triques
- Score moyen par Ã©pisode
- Taux de rÃ©ussite (ex : FrozenLake)
- Temps dâ€™exÃ©cution
- Nombre de simulations

---

## ğŸ”¶ Phase 2 â€” Q-Learning (tabulaire)

### âœ… Objectif
ImplÃ©menter une approche **Reinforcement Learning** avec Q-table pour comparer Ã  MCTS.

### ğŸ“Œ TÃ¢ches
- ImplÃ©mentation du **Q-Learning** tabulaire
- Politique Îµ-greedy
- Exploration de paramÃ¨tres (taux dâ€™apprentissage, Îµ, discount)

### ğŸ“ Environnements :
- Principalement `FrozenLake-v1`
- Optionnel : CartPole avec discretisation des Ã©tats

### ğŸ”¬ Ã‰valuation
- Convergence de la Q-table
- Politique apprise vs MCTS
- Score final moyen
- Nombre dâ€™Ã©pisodes pour atteindre la stabilitÃ©

---

## ğŸ”¶ Phase 3 â€” MCTS optimisÃ© (Ã  partir dâ€™un article de recherche)

### âœ… Objectif
AmÃ©liorer lâ€™algorithme MCTS de la Phase 1 en se basant sur un **papier de recherche externe** non traitÃ© en cours. : Optimized Monte Carlo Tree Search for Enhanced Decision Making in the FrozenLake Environment
Esteban Aldana
Computer Science Department, Universidad del Valle de Guatemala Email: ald20591@uvg.edu.gt



### ğŸ“Œ Exemples dâ€™amÃ©liorations possibles :
- Playout biaisÃ© : Îµ-greedy, heuristique, ou basÃ©e sur valeur estimÃ©e `V(s)`
- MCTS avec politique adaptative ou mÃ©moire statistique
- Pruning ou rÃ©utilisation partielle dâ€™arbre

### ğŸ”¬ Comparaison :
- Flat MCTS vs MCTS amÃ©liorÃ©
- Ã‰valuation sur `FrozenLake-v1`
- Impact sur taux de rÃ©ussite et nombre de simulations nÃ©cessaires

---

## ğŸ”¶ Phase 4 â€” MCTS avec arbre (UCT)

### âœ… Objectif
ImplÃ©menter le **MCTS complet avec UCT** (Upper Confidence Bounds applied to Trees).

### ğŸ“Œ TÃ¢ches
- ImplÃ©mentation des 4 phases classiques :
  - Selection (UCB)
  - Expansion
  - Simulation
  - Backpropagation
- ParamÃ¨tre de rÃ©gulation `c`

### ğŸ”¬ Analyse :
- Profondeur dâ€™arbre
- QualitÃ© des actions sÃ©lectionnÃ©es
- Comparaison UCT vs Flat MCTS vs RL

---

## ğŸ“Š Ã‰valuations & Analyses

| MÃ©thode              | FrozenLake | CartPole | LunarLander |
|----------------------|------------|----------|-------------|
| Flat MCTS (Phase 1)  | âœ…          | âœ…        | âœ…           |
| Q-Learning (Phase 2) | âœ…          | âš ï¸        | âŒ           |
| MCTS amÃ©liorÃ© (P3)   | âœ…          | ğŸš«        | ğŸš«           |
| UCT MCTS (Phase 4)   | âœ…          | âœ…        | âœ…           |

### ğŸ“ˆ Mesures :
- Taux de rÃ©ussite
- Score moyen
- Temps de convergence
- Courbes de learning pour Q-learning

---

## ğŸ“ Structure du rapport

1. **Introduction**
   - Motivation
   - PrÃ©sentation des mÃ©thodes (MCTS, RL)

2. **MÃ©thodes**
   - Flat MCTS
   - Q-Learning
   - MCTS optimisÃ©
   - MCTS avec UCT

3. **Environnements**
   - Description et spÃ©cificitÃ©s des jeux
   - Choix de paramÃ¨tres

4. **ExpÃ©riences**
   - RÃ©sultats dÃ©taillÃ©s
   - Graphiques (score, temps, convergence)
   - Rendus visuels ou captures

5. **Discussion**
   - Comparaisons croisÃ©es
   - Limites et efficacitÃ©
   - Cas oÃ¹ MCTS/RL fonctionne mieux

6. **Conclusion & perspectives**
   - Que faire ensuite ? (Deep RL, AlphaZero-like MCTS, gÃ©nÃ©ralisation)

---

## ğŸ§  Extensions possibles (bonus)
- Ajouter Mujoco (bonus ambitieux)
- Visualisation dâ€™arbre MCTS (textuelle ou graphique)
- Analyse des politiques apprises
